import torch
import os
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments
from huggingface_hub import login

HF_TOKEN = os.getenv("HF_TOKEN")

login(token=HF_TOKEN)

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
max_seq_length = 2048
dtype = None # None –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
load_in_4bit = True # –ò—Å–ø–æ–ª—å–∑—É–µ–º 4-–±–∏—Ç–Ω—É—é –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/gemma-3-270m-unsloth-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

# 2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = False,
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

# 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ JSON
# –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
prompt_template = """### Instruction:
–û–ø—Ä–µ–¥–µ–ª–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç —Ä–µ–∫–ª–∞–º–æ–π. –û—Ç–≤–µ—Ç—å "1", –µ—Å–ª–∏ —ç—Ç–æ —Ä–µ–∫–ª–∞–º–∞, –∏ "0", –µ—Å–ª–∏ –Ω–µ—Ç.

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token
def formatting_prompts_func(examples):
    inputs     = examples["text"]
    outputs    = examples["result"]
    texts = []
    for input_text, output_text in zip(inputs, outputs):
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –ø–æ —à–∞–±–ª–æ–Ω—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏
        text = prompt_template.format(input_text, output_text) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

# –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Ñ–∞–π–ª
dataset = load_dataset("json", data_files="dataset.json", split="train")
dataset = dataset.map(formatting_prompts_func, batched = True,)

# 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 8,
        gradient_accumulation_steps = 1,
        warmup_steps = 5,
        num_train_epochs = 2,
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

print("Starting training...")
trainer.train()
print("Training finished.")

# 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
print("Saving LoRA adapters to 'lora_model'...")
model.save_pretrained("lora_model")
print("Model saved successfully.")

# 6. –ü—Ä–∏–º–µ—Ä –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
print("\nRunning inference example...")
from transformers import TextStreamer

# –ó–ê–ü–û–õ–ù–ò–¢–ï –≠–¢–û –ü–û–õ–ï –í–ê–®–ò–ú –¢–ï–ö–°–¢–û–ú
input_text = """
"MEBELUX" (https://t.me/+emfagIYEsYViZGRi) - –º–µ–±–µ–ª—å–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ —Å –±–µ–∑—É–ø—Ä–µ—á–Ω–æ–π —Ä–µ–ø—É—Ç–∞—Ü–∏–µ–π.
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π, —É–¥–æ–±–Ω–æ–π –∏ –Ω–∞–¥–µ–∂–Ω–æ–π –º–µ–±–µ–ª–∏, —Å–æ–∑–¥–∞–Ω–Ω–æ–π –¥–ª—è –±–µ—Å–ø—Ä–æ–±–ª–µ–º–Ω–æ–π —Å–ª—É–∂–±—ã.

–ü–æ—á–µ–º—É –≤—ã–±–∏—Ä–∞—é—Ç MEBELUX? (https://t.me/+emfagIYEsYViZGRi)
‚óΩ25 –ª–µ—Ç –æ–ø—ã—Ç–∞
‚óΩ–≠–∫–æ–Ω–æ–º–∏—è –¥–æ 50% ‚Äì –±–µ–∑ –ø–æ—Å—Ä–µ–¥–Ω–∏–∫–æ–≤ –∏ —Ä–æ–∑–Ω–∏—á–Ω—ã—Ö –Ω–∞—Ü–µ–Ω–æ–∫
‚óΩ–°—Ä–æ–∫ –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è 2-4 –Ω–µ–¥–µ–ª–∏
‚óΩ–ß–∏—Å—Ç–æ—Ç–∞ –∏ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–æ–Ω—Ç–∞–∂–µ
‚óΩ–ü—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –æ—Ç 10%
‚≠ê –ë–µ—Å–ø–ª–∞—Ç–Ω–æ: –∑–∞–º–µ—Ä, –ø—Ä–æ–µ–∫—Ç, –¥–æ—Å—Ç–∞–≤–∫–∞!

‚úçÔ∏è –ü–æ–¥–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞ "MEBELUX" (https://t.me/+emfagIYEsYViZGRi)
ü§ç –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –æ—Ç–∑—ã–≤—ã (https://t.me/MEBELYX/564) –∫–ª–∏–µ–Ω—Ç–æ–≤
üßÆ –û—Å—Ç–∞–≤—å—Ç–µ –∑–∞—è–≤–∫—É –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ä–∞—Å—á–µ—Ç: @mebelsakas
üìû WhatsApp / –ó–≤–æ–Ω–æ–∫: +7(927)093-37-88
"""

prompt = prompt_template.format(
    input_text, # –¢–µ–∫—Å—Ç –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    "",         # –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
)

inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
text_streamer = TextStreamer(tokenizer)

print(f"Generating response for input: '{input_text}'")
_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256)
